\chapter{Эксперименты}
\label{chapter:experiments}
В данной главе представлена экспериментальная часть работы. Сравнение результатов работы алгоритма выявления предпочтений \enquote{при прочих равных} с другими методами обучения предпочтениям является ключевой частью данного исследования.

Глава состоит из четырех разделов. В первом описаны наборы данных, на которых сравнивались алгоритмы. Во втором разделе описаны сами алгоритмы, которые были рассмотрены в данной работе. В третьем описана методика сравнения алгоритмов. И в заключительной части представлены результаты экспериментов.

\section{Рассматриваемые алгоритмы}

Как описано в Введении, первоочередная задача данной работы – сравнение Алгоритма \ref{algo:prediction} с другими методами машинного обучения. В сравнении участвуют методы классифицирующих деревьев, а так же Байесовские методы. В разделах \ref{subsec:c4.5} -- \ref{subsec:bayes_net} подробно описаны эти алгоритмы.

	\subsection{C4.5}
	\label{subsec:c4.5}
	
	\subsection{Наивный Байесовский классификатор}
	\label{subsec:naive_bayes}
	
	\subsection{Байесовская сеть}
	\label{subsec:bayes_net}

\section{Входные данные}
	В данной работе представлена
	апробация Алгоритма проводилась на двух наборах данных: набор данных о пользовательских предпочтениях в автомобилях и набор данных о предпочтениях в суши. Ниже подробно описан каждый из наборов данных.
	
	%TODO: перенести в секцию с описанием алгоритма
	%\subsection{Искусственный набор данных}
		%Искусственный набор взят из \cite{Obiedkov:2013}. Этот набор данных состоит из семи объектов, пять из которых представлены на рис.~\ref{fig:pcxt}. Еще два объекта: $c_6={minivan, red, bright}$ и $c_7={SUV, red, bright}$. Эти данные не много могут сказать о качестве работы алгоритма, но их можно использовать для базовой проверки его возможностей.
	
	\subsection{Автомобили}
		Первый реальный набор данных собран Аббаснеджадом и др. для \cite{dataset:Abbasnejad:2013}. Для сбора пользовательских предпочтений авторы использовали краудсорсинговую платформу Amazon Mechanical Turk\footnote{mturk.com – информация о проекте Amazon Mechanical Turk}. Набор данных состоит из двух экспериментов\footnote{Данные размещены по адресу: http://users.cecs.anu.edu.au/~u4940058/CarPreferences.html}: в первом была собрана информация от 60 пользователей о 10 машинах, во втором – от 60 пользователей о 20 машинах (далее – CD1 и CD2 обозначают первый и второй эксперимент, соответственно). Причем в CD2 информации о каждой из машин было больше. Далее подробно описана информация, представленная в наборе данных.
		
		\vspace{1em}
		
		\noindent Характеристики пользователей:
		\vspace{-0.7em}
		\begin{itemize}[itemsep=-1.5mm]
			\item ID: уникальный идентификатор пользователя
			\item Образование: отсутствие ответа (0), старшая школа (1), бакалавриат (2), PhD (3)
			\item Возраст: отсутствие ответа (0), меньше 25 (1), от 25 до 30 (2), от 30 до 35 (3), больше 40 (4)
			\item Пол: отсутствие ответа (0), мужской (1), женский (2)
			\item Регион: отсутствие ответа (0), юг (1), запад (2), северо-восток (3), средний запад (4)
			\item Количество правильных ответов на контрольные вопросы: 3, 4 или 5
		\end{itemize}
		Данные собирались среди американской аудитории. Каждому пользователю было задано по пять контрольных вопросов, каждый из которых является одним из пользовательских предпочтений в обратном порядке (например, если пользователь указал, что $o_1 < o_5$, то контрольный вопрос имеет вид $o_5 < o_1?$, где $o_1$ и $o_5$ – какие-то объекты). На основе количества правильных ответов на контрольные вопросы можно судить о согласованности пользовательских предпочтений.
		
		\vspace{1em}
		
		\noindent Признаки машин:
		\vspace{-0.7em}
		\begin{enumerate}[itemsep=-1.5mm]
			\item Тип кузова: седан (1), SUV\footnote{SUV (англ. Sport Utility Vehicle или Suburban Utility Vehicle) – подобие внедорожника} (2), хэтчбек (3)
			\item Коробка передач: ручная (1), автоматическая (2)
			\item Объем двигателя: 2.5L, 3.5L, 4.5L, 5.5L, 6.2L
			\item Тип топлива: гибрид (1), не гибрид (2)
			\item Количество ведущих колес: все колеса ведущие (AWD, 4x4) (1), передние колеса ведущие (FWD) (2)
		\end{enumerate} 
		В первом эксперименте не были представлены хэтчбеки, а так же не использовалась информация о количестве ведущих колес.
	
	\subsection{Суши}
		Набор данных собран Камишимой и Акахо и использован ими в \cite{Kamishima:2003}, \cite{Kamishima:2006} и других работах. Авторы опрашивали 5000 человек об их предпочтениях в суши (всего 100 наименований). При сборе данных авторы просили пользователей заполнить несколько опросников, в результате чего были сформированы 3 набора данных\footnote{Набор данных о предпочтениях суши доступен по адресу: http://www.kamishima.net/sushi/}:
		\begin{enumerate}[itemsep=-1.5mm]
			\item Выбрав 10 наиболее популярных суши, авторы попросили каждого из участников проранжировать их. В результате получен список из 5000 ранжирований. (в дальнейшем этот набор данных обозначается как SDa)
			\item Выбирая 10 случайных суши из 100 (выбор не равновероятностный, он основан на популярности суши), авторы предлагали каждому из пользователей их проранжировать. (в дальнейшем этот набор данных обозначается как SDb)
			\item Используя тот же набор суши (10 из 100), участники опроса должны были поставить каждой из альтернатив оценку от 0 до 4, включительно. В работе эти данные не используются.
		\end{enumerate}
		
		В связи со спецификой японской культуры еды, авторы подробно указывают место жительства каждого из участников, а так же место его жительства до 15 лет. На основе этой информации для выявления предпочтений можно использовать методы коллаборативной фильтрации\cite{Ricci:2011}, одну из вариаций которой авторы используют в \cite{Kamishima:2003}.
		
		\vspace{1em}
		
		\noindent Характеристики пользователей:
		\vspace{-0.7em}
		\begin{enumerate}[itemsep=-1.5mm]
			\item ID: уникальный идентификатор пользователя
			\item Пол: мужской (0), женский (1)
			\item Возраст: от 15 до 19 (0), от 20 до 29 (1), от 30 до 39 (2), от 40 до 49 (3), от 50 до 59 (4), больше 60 (5)
			\item Количество времени (секунд), которое ушло у участника на заполнение анкеты
			\itemrange{6} Признаки, связанные с местом проживания человека
		\end{enumerate}
		
		\noindent Признаки суши:
		\vspace{-0.7em}
		\begin{enumerate}[itemsep=-1.5mm]
			\item ID: уникальный идентификатор суши
			\item Название
			\item Стиль: маки (0), другое (1)
			\item Группа: морская еда (0, соответствует подгруппам 0--8), другое (1)
			\item Подгруппа: синекожая рыба (0), красная рыба (1), \dots, овощи (11) 
			\item Жирность: диапазон [0-4], где 0 -- наибольшая жирность
			\item Частота употребления этого вида суши участниками опроса: диапазон [0-3], где 3 -- наибольшая частота
			\item Цена (нормализованная)
			\item Частота продажи суши: диапазон [0-1], где 1 -- наибольшая частота
		\end{enumerate}
	

\section{Проведение экспериментов}
	При проведении экспериментов использовался метод перекресной проверки ($k$-fold cross-validation)\cite{Hastie:2001}. В одной половине экспериментов исходные данные делились на $n$ частей, где $n$ – количество объектов; в другой половине частей было $\frac{n}{2}$, то есть на каждом шаге тестирующий набор данных состоял из двух элементов. В разделах \ref{subsec:exp_cars} -- \ref{subsec:exp_sushi} подробно описаны проведенные эксперименты, а так же представлены их результаты.
	
	Каждый из экспериментов над объектами $O = \{o_1, o_2, \dots, o_n\}$ включал следующие действия:
	\begin{enumerate}
		\item Обучающая выборка $T = O \setminus \{o_i\}$, где $i$ при каждой итерации увеличивается на 1, проходя от 1 до $n$, включительно;
		\item Для каждого $o_j \in T$ с помощью тестируемого алгоритма проверить $o_j \leq o_i$ и $o_j \geq o_i$:
		\begin{enumerate}
			\item Если алгоритм утверждает $o_j \leq o_i$, хотя на самом деле (опираясь на ответы опрашиваемых пользователей) $o_j \geq o_i$, зачислить 1 балл штрафа. Также штраф начисляется и в обратной ситуации: если алгоритм вывел $o_j \geq o_i$, хотя на самом деле $o_j \leq o_i$;
			\item Если алгоритм с \emph{равной} степенью уверенности утверждает и $o_j \leq o_i$, и $o_j \geq o_i$, и при этом пользователь однозначно склонялся к одному из вариантов, начисляется 0.5 баллов штрафа.
		\end{enumerate} 
		\item Находится среднее из штрафов для каждого $(o_j, o_i) \in (T, o_i)$. Это среднее, деленное на $n-1$, становится показателем точности для $o_i$\footnote{Как видно из описания эксперимента, максимальный штраф, который может получить каждый $o_j \in T$, равен $n-1$. Это получается, если испытываемый алгоритм на каждый запрос отвечает некорректно. Таким образом, если поделить средний штраф по всем $o_j \in T$ на $n-1$, мы получим долю правильных ответов для текущего $o_i$};
		\item После $n$ итераций получено $n$ показателей штрафов (для каждого из объектов $O$). Далее по этим штрафам строится статистика.
	\end{enumerate}
	
	\subsection{Автомобили}
	\label{subsec:exp_cars}
		
		При проведении экспериментов (описанным выше методом) над \textbf{алгоритмом выявления предпочтений \enquote{при прочих равных}}, реализованным ``как есть'' (согласно описанию раздела \ref{subsec:cp_description}), точность предсказаний оказывается на низком уровне (см. табл.~\ref{tbl:cars_results}).
		Недостаток данного подхода в том, что при использовании Алгоритма \ref{algo:prediction} с реальными данными, практически для любой пары объектов $(o_1,o_2)$ можно найти и пару объектов $(g_1,h_1)$, которая поддерживает утверждение $o_1 \leq o_2$, и пару $(g_2,h_2)$, которая поддерживает противоположное предпочтение – $o_1 \geq o_2$. В таблицах с результатами экспериментов эта реализация обозначена \emph{CP} (Ceteris Paribus). 
		
		Принимая во внимание сказанное, дополнительно было реализовано 3 модификации Алгоритма \ref{algo:prediction}, которые позволяют предсказывать пользовательские предпочтения с более высокой точностью.
		Первая модификация заключается в подсчете количества различных пар $(g,h)$, которые ``поддерживают'' каждое из предпочтений $o_j \leq o_i$ и $o_j \geq o_i$. Таким образом, Алгоритм~\ref{algo:prediction} был изменен в Алгоритм~\ref{algo:CPe}. Для каждых $(o_j, o_i)$ Алгоритм~\ref{algo:CPe} ищет поддержки для $A:=o_j',\: B:=o_i'$ и для $A:=o_i',\: B:=o_j'$. Если первая больше второй, то вывод $o_j \leq o_i$; если вторая больше первой, то $o_i \leq o_j$; иначе альтернативы равнозначны. В дальнейшем этот алгоритм обозначается \emph{CPs} (Cetris Paribus with Support).
		
		\begin{definition}
			\emph{Поддержка} – для данных $A$ и $B$, количество различных пар $(g, h)$, для которых выполняется $\PP \models \DEF$, где $D := A \cap g'$, $E := B \cap h'$ и $F := (M \setminus (A \vartriangle B)) \cap (M \setminus (g' \vartriangle h'))$\footnote{Согласно опр.~\ref{def:context}, $M$ – множество всех признаков.}.
		\end{definition}
		
		\begin{algorithm}
			\caption{\algname{Вычисление поддержки}$(A, B, \PP)$ (основано на Алг.~\ref{algo:prediction})}
			\label{algo:CPe}
			\begin{algorithmic}[1]
				\REQUIRE Содержания объектов $A, B \subseteq M$ и контекст предпочтений $\PP = (G, M, I, \leq)$.
				\ENSURE поддержка $A, B$
				\item[]
				\STATE $S := \emptyset$
				\FORALL{$g \in G$}
				\STATE $D := A \cap g'$
				\FORALL{$h \in G \setminus \{g\}$ таких, что $g \leq h$}
				\STATE $E := B \cap h'$
				\STATE $F := (M \setminus (A \vartriangle B)) \cap (M \setminus (g' \vartriangle h'))$
				\IF{$\PP \models \DEF$}
				\STATE $S := S \cup (g, h)$
				\ENDIF
				\ENDFOR
				\ENDFOR
				\RETURN $|S|$
			\end{algorithmic}
		\end{algorithm}
		
		Вторая модификация Алгоритма~\ref{algo:prediction} основана на поиске максимальной поддержки для фиксированных $DFE$. Для пары объектов $(o_j, o_i)$ находятся все тройки $DFE_{(i,j)}$, поддерживающие $o_j \leq o_i$, а так же все тройки $DFE_{(j,i)}$, поддерживающие $o_i \leq o_j$. Затем для каждой тройки считается максимальная поддержка. Процесс аналогичен Алгоритму~\ref{algo:CPe}, за исключением того, что строка~8 выполняется лишь для тех $D$, $F$ и $E$, которые совпадают с фиксированными $DFE$. Таким образом, предсказание предпочтения основано на следующей формуле:
		
		\begin{equation}
		\label{eq:CPo}
			\max_{t\, \in DFE_{(i,j)}}\#(g,h) \: - \max_{t\, \in DFE_{(j,i)}}\#(g,h)
		\end{equation}
		где $t$ – одна из троек $DFE$, которая поддерживает $o_j \leq o_i$ или $o_i \leq o_j$\\
		$\#(g,h)$ – количество различных пар $(g,h)$.\\
		Если результат \ref{eq:CPo} положителен, значит $o_i \leq o_j$; если отрицателен, то $o_j \leq o_i$; иначе альтернативы равнозначны. Данная модификация алгоритма обозначается \emph{CPfs} (Ceteris Paribus with Fixed Support).
		
		Третья модификация Алгоритма~1 является смешением двух вышеописанных подходов. В случае, если \ref{eq:CPo} возвращает ноль, срабатывает подход, основанный на расчете поддержки (см. Алгоритм~\ref{algo:CPe}).
		
		С использованием \textbf{алгоритма C4.5} было проведено 3 типа экспериментов. В первом варианте каждая из строк имеет вид: 
		\begin{equation}
		\label{eq:c4.5_unpaired_row}
		\{att_1^l, att_1^r, att_2^l, att_2^r, \dots, att_n^l, att_n^r, [< | >]\}
		\end{equation}
		где $att_i^l \in \{\text{YES}, \text{NO}\} \quad \forall i = \overline{1..n}$ (аналогично для $att_i^r$); \\
		$att_i^l$ ($att_i^r$) показывает наличие или отсутствие признака $i$ у левого (правого) объекта из пары; \\
		последний элемент строки показывает класс пары: правый объект предпочитается левому (в случае $<$) или левый правому (в случае $>$). \\
		Как видно, в данном случае каждая строка обучающей выборки содержит $2n + 1$ элементов, где $n$ – количество всех признаков. Далее этот алгоритм обозначается как \emph{C4.5 unpaired}.
		
		Во втором варианте экспериментов с алгоритмом C4.5 в каждой строке обучающей выборки признаки сгруппированы по их типу (например, ``тип кузова''). Таким образом, строки имеют вид:
		\begin{equation}
		\label{eq:c4.5_paired_row}
		\{(att_{[1]}^l, att_{[1]}^r), (att_{[2]}^l, att_{[2]}^r), \dots, (att_{[k]}^l, att_{[k]}^r), [< | >]\}
		\end{equation}
		где каждая из пар $(att_{[i]}^l, att_{[i]}^r)$ показывает признаки типа $i$ (например, признаком $i=2$ может быть ``цвет интерьера'') левого и правого объектов пары предпочтения. \\
		Как видно, в данном случае каждая строка обучающей выборки содержит $k + 1$ элементов, где $k$ – количество категорий признаков. \emph{Стоит отметить, что в данном примере накладываются дополнительные ограничения на входные данные. Если первая адаптация алгоритма C4.5 допускала наличие любых признаков у объекта, то эта версия допускает лишь одно значение для каждой из категорий признаков (например, автомобиль может быть или седаном, или хэтчбеком, но не одновременно, что вполне соответствует реальности).} Далее этот вариант использования алгоритма обозначается как \emph{C4.5 paired}.
		
		Наконец, третий вариант использования алгоритма C4.5 основан на его возможности работать с числовыми признаками. В этом случае каждая из строк имеет тот же вид, что и \eqref{eq:c4.5_paired_row}, за исключением числовых категорий (в случае автомобилей это тип двигателя). Для числовых категорий вместо пары указана разность числовых показателей левого и правого объекта пары предпочтения. Например, в случае автомобильных двигателей вместо пары $(2.5L, 4.5L)$ будет указано $-2$. Далее этот вариант использования алгоритма обозначается как \emph{C4.5 paired, numeric}. 
		
		Наивный \textbf{Байесовский классификатор} (далее – \emph{Naive Bayes}), а так же классификатор, основанный на \textbf{Байесовской сети} (далее – \emph{Bayes Net}), используют представление данных \eqref{eq:c4.5_unpaired_row}. 
		
		Результаты экспериментов, проведенные над набором данных с предпочтениями в автомобилях, представлены в табл.~\ref{tbl:cars_results}. %В первом столбце указано название эксперимента, во втором – указан процент правильно распознанных 
	
	\begin{table}[hb]
		\centering
		\tablecaption{Автомобили: результаты экспериментов}
		\begin{tabular}{|l|c|c|}
			\hline
			Метод   & 1-out accuracy \% & 2-out accuracy \% \rule{0pt}{2.4ex} \\
			\hline  
			CP & $59.09 \pm 3.08$ & $52.94 \pm 0.97$ \rule{0pt}{2.4ex} \\ 
			CPs & $84.52 \pm 8.7$ & $82.97 \pm 10.74$ \\
			CPfs & $65.73 \pm 4.15$ & $60.9 \pm 4.56$ \\
			CPm & $69.98 \pm 5.07$ & $62.11 \pm 4.66$ \\ 
			C4.5 unpaired & $80.44 \pm 7.82$ & $71.61 \pm 8.97$ \\ 
			C4.5 paired  & $83.35 \pm 8.74$ & $77.53 \pm 10.48$ \\ 
			C4.5 paired, numeric & $86.79 \pm 6.83$ & $84.27 \pm 9.15$ \\
			Naive Bayes  & $83.76 \pm 7.2$ & $82.69 \pm 9.68$ \\ 
			Bayes Net  & $83.6 \pm 6.98$ & $82.26 \pm 10.31$ \\ 
			\hline
		\end{tabular}
		\label{tbl:cars_results}
	\end{table}
	
	%TODO: точность и полнота
	%TODO: confusion matrix?
	
	\subsection{Суши} 
	\label{subsec:exp_sushi}

	Используя терминологию Стивенса из \cite{Stevens:1951}, мы используем...

\section{Реализация}
	Написано на Java 8. Используется Weka. Еще используется Apache Commons Math.
